2024-12-28 16:03:37,617 - inference_qa_new - INFO - running ../src/inference_qa_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_5.jsonl.gz --output_path ../result/mdqa_result/mdqa_10documents.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --enable_changed_rope --sample_num 100 --batch_size 6 --answer_idx 5 --narrow_scale 1 --boost_scale 1
2024-12-28 16:04:36,851 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2024-12-28 16:07:28,485 - eval_qa_response - INFO - Computing metrics
2024-12-28 16:07:28,491 - eval_qa_response - INFO - best_subspan_em: 0.59
