2025-01-20 20:12:20,486 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_10.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 10 --enable_changed_rope
2025-01-20 20:13:31,459 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 20:13:31,693 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 20:13:31,733 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 20:34:04,705 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_1.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 1 --enable_changed_rope
2025-01-20 20:34:35,390 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-20 20:37:31,717 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_10.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 10 --enable_changed_rope
