2025-01-10 11:55:13,430 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 11:55:34,106 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 11:56:04,747 - logging - WARNING - Instantiating LlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.
2025-01-10 11:57:01,100 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 11:57:31,897 - logging - WARNING - Instantiating LlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.
2025-01-10 12:01:23,262 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 12:01:53,830 - logging - WARNING - Instantiating LlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.
2025-01-10 12:03:36,567 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 12:04:07,187 - logging - WARNING - Instantiating LlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.
2025-01-10 12:04:27,864 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-10 12:07:46,439 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 12:08:16,970 - logging - WARNING - Instantiating LlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.
2025-01-10 12:08:41,501 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-10 12:09:46,322 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 12:10:42,197 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-10 12:12:07,413 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 12:12:17,149 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 4 --answer_idx 5 --enable_changed_rope
2025-01-10 12:13:12,472 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-10 12:37:34,361 - eval_qa_response - INFO - Computing metrics
2025-01-10 12:37:34,402 - eval_qa_response - INFO - best_subspan_em: 0.59
