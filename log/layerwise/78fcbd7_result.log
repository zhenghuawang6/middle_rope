2025-01-19 18:14:17,474 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_1.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 1 --enable_changed_rope
2025-01-19 18:14:57,544 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:14:57,627 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:14:57,642 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:14:57,671 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:23:46,329 - eval_qa_response - INFO - Computing metrics
2025-01-19 18:23:46,375 - eval_qa_response - INFO - best_subspan_em: 0.632
2025-01-19 18:24:09,844 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_3.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 3 --enable_changed_rope
2025-01-19 18:24:53,120 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:24:53,356 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:24:53,356 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:24:53,369 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:33:15,675 - eval_qa_response - INFO - Computing metrics
2025-01-19 18:33:15,720 - eval_qa_response - INFO - best_subspan_em: 0.592
2025-01-19 18:33:32,386 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_10.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 10 --enable_changed_rope
2025-01-19 18:34:03,987 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:34:04,049 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:34:04,274 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:34:04,287 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 18:42:09,537 - eval_qa_response - INFO - Computing metrics
2025-01-19 18:42:09,580 - eval_qa_response - INFO - best_subspan_em: 0.606
2025-01-19 18:43:03,601 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_1.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 1
2025-01-19 18:50:23,281 - eval_qa_response - INFO - Computing metrics
2025-01-19 18:50:23,327 - eval_qa_response - INFO - best_subspan_em: 0.594
2025-01-19 18:50:38,831 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_3.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 3
2025-01-19 18:57:59,378 - eval_qa_response - INFO - Computing metrics
2025-01-19 18:57:59,425 - eval_qa_response - INFO - best_subspan_em: 0.582
2025-01-19 18:58:14,641 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_10.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 10
2025-01-19 19:05:06,816 - eval_qa_response - INFO - Computing metrics
2025-01-19 19:05:06,858 - eval_qa_response - INFO - best_subspan_em: 0.602
2025-01-19 19:43:39,052 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5
2025-01-19 19:52:18,606 - eval_qa_response - INFO - Computing metrics
2025-01-19 19:52:18,650 - eval_qa_response - INFO - best_subspan_em: 0.556
2025-01-19 19:52:32,974 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_7.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 7
2025-01-19 19:59:36,041 - eval_qa_response - INFO - Computing metrics
2025-01-19 19:59:36,085 - eval_qa_response - INFO - best_subspan_em: 0.554
2025-01-19 20:00:11,065 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-19 20:00:57,715 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:00:57,750 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:00:58,038 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:00:58,046 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:32:29,917 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-19 20:33:13,087 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:33:13,133 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:33:13,340 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:33:13,362 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:41:44,399 - eval_qa_response - INFO - Computing metrics
2025-01-19 20:41:44,445 - eval_qa_response - INFO - best_subspan_em: 0.576
2025-01-19 20:41:59,813 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_7.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 7 --enable_changed_rope
2025-01-19 20:42:31,969 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:42:31,969 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:42:32,299 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:42:32,304 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 20:51:10,888 - eval_qa_response - INFO - Computing metrics
2025-01-19 20:51:10,931 - eval_qa_response - INFO - best_subspan_em: 0.598
2025-01-19 21:04:58,625 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_1.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 1 --enable_changed_rope
2025-01-19 21:05:30,379 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:05:30,446 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:05:30,686 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:05:30,686 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:14:18,186 - eval_qa_response - INFO - Computing metrics
2025-01-19 21:14:18,231 - eval_qa_response - INFO - best_subspan_em: 0.64
2025-01-19 21:14:33,590 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_3.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 3 --enable_changed_rope
2025-01-19 21:15:06,700 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:15:06,926 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:15:06,941 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:15:06,978 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:23:27,735 - eval_qa_response - INFO - Computing metrics
2025-01-19 21:23:27,779 - eval_qa_response - INFO - best_subspan_em: 0.592
2025-01-19 21:23:42,630 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-19 21:24:14,699 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:24:14,709 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:24:15,040 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:24:15,040 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:32:57,280 - eval_qa_response - INFO - Computing metrics
2025-01-19 21:32:57,326 - eval_qa_response - INFO - best_subspan_em: 0.58
2025-01-19 21:33:12,482 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_7.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 7 --enable_changed_rope
2025-01-19 21:33:59,323 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:33:59,380 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:33:59,536 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:33:59,569 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:42:15,181 - eval_qa_response - INFO - Computing metrics
2025-01-19 21:42:15,227 - eval_qa_response - INFO - best_subspan_em: 0.6
2025-01-19 21:42:30,644 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_10.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 10 --enable_changed_rope
2025-01-19 21:43:11,017 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:43:11,208 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:43:11,208 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:43:11,238 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:50:57,497 - eval_qa_response - INFO - Computing metrics
2025-01-19 21:50:57,542 - eval_qa_response - INFO - best_subspan_em: 0.602
2025-01-19 21:55:00,567 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_1.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 1 --enable_changed_rope
2025-01-19 21:55:45,907 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:55:46,030 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:55:46,073 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 21:55:46,091 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 22:04:39,298 - eval_qa_response - INFO - Computing metrics
2025-01-19 22:04:39,343 - eval_qa_response - INFO - best_subspan_em: 0.634
2025-01-19 22:04:54,150 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_3.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 3 --enable_changed_rope
2025-01-19 22:05:29,301 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 22:05:29,351 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 22:05:29,375 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 22:05:29,826 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-19 22:16:09,440 - eval_qa_response - INFO - Computing metrics
2025-01-19 22:16:09,488 - eval_qa_response - INFO - best_subspan_em: 0.606
2025-01-19 22:16:25,261 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-19 22:16:51,985 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-19 22:17:04,778 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_7.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 7 --enable_changed_rope
2025-01-19 22:17:18,679 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_10.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 10 --enable_changed_rope
2025-01-19 22:30:26,989 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-19 22:30:40,285 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_7.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 7 --enable_changed_rope
