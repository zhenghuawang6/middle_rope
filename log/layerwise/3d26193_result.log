2025-01-20 19:46:06,762 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_1.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 1 --enable_changed_rope
2025-01-20 19:47:33,415 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 19:47:33,416 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 19:47:33,416 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 19:59:20,849 - eval_qa_response - INFO - Computing metrics
2025-01-20 19:59:20,891 - eval_qa_response - INFO - best_subspan_em: 0.624
2025-01-20 19:59:37,589 - inference_qa_layerwise_new - INFO - running ../src/inference_qa_layerwise_new.py --input_path ../data/mutiqa/generated_data/nq-open-10_total_documents_gold_at_0.jsonl.gz --output_path ../result/layerwise/mdqa_10documents_5.json --model_name lmsys/vicuna-7b-v1.5 --apply_layers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31 --seed 42 --sample_num 500 --batch_size 2 --answer_idx 5 --enable_changed_rope
2025-01-20 20:00:35,133 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 20:00:35,213 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 20:00:35,225 - logging - WARNING - The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2025-01-20 20:12:05,109 - eval_qa_response - INFO - Computing metrics
2025-01-20 20:12:05,154 - eval_qa_response - INFO - best_subspan_em: 0.574
